#     ,-.  ;-.  ,--. .  . .   , ,--. ,-.   ,-.  ,--.    ,-.  ,-.  .  . ,--. ,  ,-.     #
#    /   \ |  ) |    |\ | |  /  |    |  ) (   ` |      /    /   \ |\ | |    | /        #
#    |   | |-'  |-   | \| | /   |-   |-<   `-.  |-     |    |   | | \| |-   | | -.     #
#    \   / |    |    |  | |/    |    |  \ .   ) |      \    \   / |  | |    | \  |     #
#     `-'  '    `--' '  ' '     `--' '  '  `-'  `--'    `-'  `-'  '  ' '    '  `-'     #

########################################################################################
# Airflow Settings
########################################################################################
# Some brand-based suggestions: #C52B9B (pink), #FFE033 (yellow)
AIRFLOW__WEBSERVER__NAVBAR_COLOR="#FFF"
# Disabled by default to make development easier
# (enabled on prod for security)
AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS=False
# Use the following python code to generate a fernet key for production
# python -c "import base64, os; print(base64.urlsafe_b64encode(os.urandom(32)).decode())"
# AIRFLOW__CORE__FERNET_KEY=
# Executor to use
AIRFLOW__CORE__EXECUTOR=LocalExecutor

########################################################################################
# API Keys
########################################################################################
WALTERS_ART_MUSEUEM_KEY=not_set
BROOKLYN_MUSEUM_API_KEY=not_set
DATA_GOV_API_KEY=not_set
EUROPEANA_API_KEY=not_set
FLICKR_API_KEY=not_set
JAMENDO_APP_KEY=not_set
NYPL_API_KEY=not_set
THINGIVERSE_TOKEN=not_set

########################################################################################
# Connection/Variable info
########################################################################################
# Airflow primary metadata database
# Change the following line in prod to use the appropriate DB
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
# Remote logging connection ID
# Replace "access_key" and "secret+key" with the real values. Secret key must be URL-encoded
AIRFLOW_CONN_AWS_DEFAULT=aws://test_key:test_secret@?region_name=us-east-1&host=http://s3:5000
# Change the following line in prod to use the appropriate DB
AIRFLOW_CONN_POSTGRES_OPENLEDGER_UPSTREAM=postgres://deploy:deploy@postgres:5432/openledger
AIRFLOW_CONN_POSTGRES_OPENLEDGER_TESTING=postgres://deploy:deploy@postgres:5432/openledger

OPENLEDGER_CONN_ID=postgres_openledger_upstream
TEST_CONN_ID=postgres_openledger_testing

S3_LOCAL_ENDPOINT=http://s3:5000
AIRFLOW_CONN_AWS_PROD=aws://test_key:test_secret@?region_name=us-east-1&host=http://s3:5000
AIRFLOW_CONN_AWS_TEST=aws://test_key:test_secret@?region_name=us-east-1&host=http://s3:5000

AWS_CONN_ID=aws_prod
AWS_TEST_CONN_ID=aws_test

AIRFLOW_CONN_EMR_EMPTY=emr://
AIRFLOW_CONN_EMR_TEST=emr://?host=http://s3:5000

EMR_CONN_ID=emr_empty
EMR_TEST_CONN_ID=emr_test


########################################################################################
# Other config
########################################################################################
# External port airflow will be mounted to
AIRFLOW_PORT=9090
# Minutes to wait until processing a file that hasn't been modified
LOADER_FILE_AGE=1
# Contact email for any APIs
CONTACT_EMAIL=openverse@wordpress.org
# This is where intermediate TSVs and other files will be written
# In production, this should be a permanent (non-temp) location
OUTPUT_DIR=/tmp/
# AWS/S3 configuration - does not need to be changed for development
AWS_ACCESS_KEY=test_key
AWS_SECRET_KEY=test_secret
TEST_ACCESS_KEY=test_key
TEST_SECRET_KEY=test_secret
# General bucket used for TSV->DB ingestion and logging
# TODO: Might need a separate bucket for logging
OPENVERSE_BUCKET=openverse-storage
# Used only for commoncrawl parsing
S3_BUCKET=not_set
