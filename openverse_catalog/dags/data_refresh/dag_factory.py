"""
# Data Refresh DAG Factory
This file contains the factory function which generates our data refresh DAGs.
These DAGs initiate a data refresh for a given media type and awaits the
success or failure of the refresh. Importantly, they are also configured to
ensure that no two data refresh DAGs can run concurrently, as required by
the server.

A data refresh occurs on the data refresh server in the openverse-api project.
This is a task which imports data from the upstream Catalog database into the
API, copies contents to a new Elasticsearch index, and makes the index "live".
This process is necessary to make new content added to the Catalog by our
provider DAGs available on the frontend.

The DAGs generated by this factory allow us to trigger those refreshes through
Airflow. Since no two refreshes can run simultaneously, all tasks are run in a
special `data_refresh` pool with a single worker slot. To ensure that tasks
run in an acceptable order (ie the trigger step for one DAG cannot run if a
previously triggered refresh is still running), each DAG has the following
steps:

1. The `wait_for_data_refresh` step uses a custom Sensor that will wait until
none of the `external_dag_ids` (corresponding to the other data refresh DAGs)
are 'running'. A DAG is considered to be 'running' if it is itself in the
RUNNING state __and__ its own `wait_for_data_refresh` step has completed
successfully. The Sensor suspends itself and frees up the worker slot if
another data refresh DAG is running.

2. The `trigger_data_refresh` step then triggers the data refresh by POSTing
to the `/task` endpoint on the data refresh server with relevant data. A
successful response will include the `status_check` url used to check on the
status of the refresh, which is passed on to the next task via XCom.

3. Finally the `wait_for_data_refresh` task waits for the data refresh to be
complete by polling the `status_url`. Note this task does not need to be
able to suspend itself and free the worker slot, because we want to lock the
entire pool on waiting for a particular data refresh to run.

You can find more background information on this process in the following
issues and related PRs:

- [[Feature] Data refresh orchestration DAG](
https://github.com/WordPress/openverse-catalog/issues/353)
"""
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import urlparse

from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.providers.http.sensors.http import HttpSensor
from data_refresh.sensors import ExternalDAGsSensor


logger = logging.getLogger(__name__)


XCOM_PULL_TEMPLATE = "{{{{ ti.xcom_pull(task_ids='{}', key='{}') }}}}"
DATA_REFRESH_POOL = "data_refresh"


# Filter for the `trigger_data_refresh` task, used to grab the endpoint needed
# to poll for the status of the triggered data refresh. This information will
# then be available via XCom in the downstream tasks.
def response_filter_data_refresh(response):
    status_check_url = response.json()["status_check"]
    return urlparse(status_check_url).path


# Response check to the `wait_for_completion` Sensor. Processes the response to
# determine whether the task can complete.
def response_check_wait_for_completion(response):
    data = response.json()

    if data["active"]:
        # The data refresh is still running. Poll again later.
        return False

    if data["error"]:
        raise AirflowException("Error triggering data refresh.")

    logger.info(
        f"Data refresh done with {data['percent_completed']}% \
        completed."
    )
    return True


def create_data_refresh_dag(
    dag_id: str,
    media_type: str,
    external_dag_ids: List[str],
    start_date: datetime = datetime(1970, 1, 1),
    default_args: Optional[Dict] = None,
    schedule_string: Optional[str] = None,
    execution_timeout: timedelta = timedelta(hours=12),
    # TODO: update
    poke_interval: int = 5,
    doc_md: str = "",
):
    """
    This factory method instantiates a DAG that will run the data refresh for
    the given `media_type`.

    A data refresh runs for a given media type in the API DB. It imports the
    data for that type from the upstream DB in the Catalog, reindexes the data,
    and updates and reindex Elasticsearch.

    A data refresh can only be performed for one media type at a time, so the DAG
    must also use a Sensor to make sure that no two data refresh tasks run
    concurrently.

    It is intended that the data_refresh tasks, or at least the initial
    `wait_for_data_refresh` tasks, should be run in a custom pool with 1 worker
    slot. This enforces that no two `wait_for_data_refresh` tasks can start
    concurrently and enter a race condition.

    Required Arguments:

    dag_id:           string giving a unique id of the DAG to be created.
    media_type:       string describing the media type to be refreshed.
    external_dag_ids: list of ids of DAG dependencies. This DAG will not run
                      concurrently with any dependent DAGs.

    Optional Arguments:

    default_args:      dictionary which is passed to the airflow.dag.DAG
                       __init__ method.
    start_date:        datetime.datetime giving the
                       first valid execution_date of the DAG.
    schedule_string:   string giving the schedule on which the DAG should
                       be run.  Passed to the airflow.dag.DAG __init__
                       method.
    execution_timeout: datetime.timedelta giving the amount of time a given data
                       pull may take.
    poke_interval:     interval in seconds, giving the time to wait between pokes
                       by the ExternalDAGsSensor to check if the DAG can continue
    doc_md:            string which should be used for the DAG's documentation markdown
    """
    dag = DAG(
        dag_id=dag_id,
        default_args=default_args,
        start_date=start_date,
        schedule_interval=schedule_string,
        catchup=False,
        doc_md=doc_md,
        tags=[f"data_refresh: {media_type}"],
    )

    with dag:
        # Wait to ensure that no other Data Refresh DAGs are running.
        wait_for_data_refresh = ExternalDAGsSensor(
            task_id="wait_for_data_refresh",
            external_dag_ids=external_dag_ids,
            check_existence=True,
            pool=DATA_REFRESH_POOL,
            dag=dag,
            poke_interval=poke_interval,
            mode="reschedule",
        )

        data_refresh_post_data = {"model": media_type, "action": "INGEST_UPSTREAM"}

        # Trigger the refresh on the data refresh server.
        trigger_data_refresh = SimpleHttpOperator(
            task_id="trigger_data_refresh",
            http_conn_id="data_refresh",
            endpoint="task",
            method="POST",
            headers={"Content-Type": "application/json"},
            data=json.dumps(data_refresh_post_data),
            response_check=lambda response: response.status_code == 202,
            response_filter=response_filter_data_refresh,
            pool=DATA_REFRESH_POOL,
            dag=dag,
        )

        # Wait for the data refresh to complete.
        wait_for_completion = HttpSensor(
            task_id="wait_for_completion",
            http_conn_id="data_refresh",
            endpoint=XCOM_PULL_TEMPLATE.format(
                trigger_data_refresh.task_id, "return_value"
            ),
            method="GET",
            response_check=response_check_wait_for_completion,
            pool=DATA_REFRESH_POOL,
            dag=dag,
        )

        wait_for_data_refresh >> trigger_data_refresh >> wait_for_completion

    return dag
